{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Reegression\n",
    "1) import library sklean, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random as rd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.linear_model.logistic import  LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data_preprocessing/train_dummy.csv')\n",
    "y=pd.read_csv('../data_preprocessing/training.csv')['IsBadBuy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Split data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Train logistic model and caculate the accuracy of training data and test data (with bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data accuracy = 0.877994832446 ; test data accuracy = 0.874720255766\n",
      "training data roc = 0.5\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept = False, C = 1e9)\n",
    "mdl = model.fit(X_train, y_train)\n",
    "score_train = mdl.score(X_train,y_train)\n",
    "score_test = mdl.score(X_test,y_test)\n",
    "print(\"training data accuracy =\",score_train,\"; test data accuracy =\",score_test)\n",
    "roc_test = roc_auc_score(y_test, model.predict(X_test),average='weighted')\n",
    "print(\"training data roc =\",roc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very high but the roc is not very good\n",
    "\n",
    "5) get of bad and good samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_reset = y_train.reset_index(drop=True)\n",
    "X_train_reset = X_train.reset_index(drop=True)\n",
    "y_good_index = []\n",
    "y_bad_index = []\n",
    "index_new=[]\n",
    "for i in range(0,len(y_train_reset)):\n",
    "    if y_train_reset[i] == 0:\n",
    "        y_good_index.append(i)\n",
    "    else:\n",
    "        y_bad_index.append(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Get a list of same number of bad and good samples \n",
    "\n",
    "  a) have repeated records; X_train_new and y_train_new are the new lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12466\n"
     ]
    }
   ],
   "source": [
    "index_new_all = []\n",
    "for i in range(0,7):\n",
    "    y_good_index_new = []\n",
    "    y_bad_index_new = []\n",
    "    index_new = []\n",
    "    rd.shuffle(y_good_index)\n",
    "    rd.shuffle(y_good_index)\n",
    "    y_good_index_new = rd.sample(y_good_index, 1000)\n",
    "    y_bad_index_new = rd.sample(y_bad_index, 1000)         \n",
    "    index_new = y_good_index_new+y_bad_index_new\n",
    "    rd.shuffle(index_new)\n",
    "    index_new_all = index_new_all+index_new\n",
    "\n",
    "y_train_new = y_train_reset[index_new_all]\n",
    "X_train_new = X_train_reset.loc[index_new_all,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "0.688142857143\n",
      "0.715779858415\n"
     ]
    }
   ],
   "source": [
    "#Not use penalty in logistic regression in repeated records\n",
    "model = LogisticRegression()\n",
    "%time \n",
    "model_nopenalty = model.fit(X_train_new, y_train_new)\n",
    "\n",
    "# check the accuracy on the training set and test set\n",
    "nop_score_train = model_nopenalty.score(X_train_new, y_train_new)\n",
    "nop_score_test = model_nopenalty.score(X_test, y_test)\n",
    "print (nop_score_train)\n",
    "print (nop_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data roc = 0.680111908995\n"
     ]
    }
   ],
   "source": [
    "roc_test = roc_auc_score(y_test, model.predict(X_test),average='weighted')\n",
    "print(\"training data roc =\",roc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  b)  have no repeated records; X_train_new2 and y_train_new2 are the new lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "0.683940317664\n",
      "0.717515414478\n"
     ]
    }
   ],
   "source": [
    "#have no repeated record\n",
    "y_good_index_new2 = rd.sample(y_good_index, 6233)\n",
    "y_bad_index_new2 = rd.sample(y_bad_index, 6233) \n",
    "index_new2 = y_good_index_new2+y_bad_index_new2\n",
    "y_train_new2 = y_train_reset[index_new2]\n",
    "X_train_new2 = X_train_reset.loc[index_new2,:]\n",
    "print(len(y_train_new2))\n",
    "\n",
    "#Not use penalty in logistic regression in records without repetition\n",
    "model2 = LogisticRegression()\n",
    "%time \n",
    "model_nopenalty2 = model2.fit(X_train_new2, y_train_new2)\n",
    "\n",
    "# check the accuracy on the training set and test set\n",
    "nop_score_train2 = model_nopenalty2.score(X_train_new2, y_train_new2)\n",
    "nop_score_test2 = model_nopenalty2.score(X_test, y_test)\n",
    "print (nop_score_train2)\n",
    "print (nop_score_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the repeated records, the accuracy is close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) logistic regression with penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.00\n",
      "Sparsity with L1 penalty: 90.97%\n",
      "training data score with L1 penalty: 0.6053\n",
      "score with L1 penalty: 0.5791\n",
      "test data roc score with L1 penalty: 0.6028\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 87.50%\n",
      "training data score with L1 penalty: 0.6766\n",
      "score with L1 penalty: 0.7366\n",
      "test data roc score with L1 penalty: 0.6736\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5682\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.02\n",
      "Sparsity with L1 penalty: 79.17%\n",
      "training data score with L1 penalty: 0.6838\n",
      "score with L1 penalty: 0.7435\n",
      "test data roc score with L1 penalty: 0.6802\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.03\n",
      "Sparsity with L1 penalty: 78.47%\n",
      "training data score with L1 penalty: 0.6870\n",
      "score with L1 penalty: 0.7463\n",
      "test data roc score with L1 penalty: 0.6849\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.04\n",
      "Sparsity with L1 penalty: 75.69%\n",
      "training data score with L1 penalty: 0.6883\n",
      "score with L1 penalty: 0.7477\n",
      "test data roc score with L1 penalty: 0.6877\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.05\n",
      "Sparsity with L1 penalty: 73.61%\n",
      "training data score with L1 penalty: 0.6880\n",
      "score with L1 penalty: 0.7476\n",
      "test data roc score with L1 penalty: 0.6898\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.06\n",
      "Sparsity with L1 penalty: 70.14%\n",
      "training data score with L1 penalty: 0.6867\n",
      "score with L1 penalty: 0.7457\n",
      "test data roc score with L1 penalty: 0.6885\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.07\n",
      "Sparsity with L1 penalty: 68.75%\n",
      "training data score with L1 penalty: 0.6873\n",
      "score with L1 penalty: 0.7464\n",
      "test data roc score with L1 penalty: 0.6889\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.08\n",
      "Sparsity with L1 penalty: 67.36%\n",
      "training data score with L1 penalty: 0.6879\n",
      "score with L1 penalty: 0.7454\n",
      "test data roc score with L1 penalty: 0.6873\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.09\n",
      "Sparsity with L1 penalty: 63.19%\n",
      "training data score with L1 penalty: 0.6888\n",
      "score with L1 penalty: 0.7450\n",
      "test data roc score with L1 penalty: 0.6870\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.10\n",
      "Sparsity with L1 penalty: 61.81%\n",
      "training data score with L1 penalty: 0.6893\n",
      "score with L1 penalty: 0.7451\n",
      "test data roc score with L1 penalty: 0.6883\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.11\n",
      "Sparsity with L1 penalty: 60.42%\n",
      "training data score with L1 penalty: 0.6897\n",
      "score with L1 penalty: 0.7451\n",
      "test data roc score with L1 penalty: 0.6890\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5682\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.12\n",
      "Sparsity with L1 penalty: 59.03%\n",
      "training data score with L1 penalty: 0.6905\n",
      "score with L1 penalty: 0.7449\n",
      "test data roc score with L1 penalty: 0.6889\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5683\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.14\n",
      "Sparsity with L1 penalty: 59.03%\n",
      "training data score with L1 penalty: 0.6902\n",
      "score with L1 penalty: 0.7437\n",
      "test data roc score with L1 penalty: 0.6886\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.15\n",
      "Sparsity with L1 penalty: 59.72%\n",
      "training data score with L1 penalty: 0.6910\n",
      "score with L1 penalty: 0.7444\n",
      "test data roc score with L1 penalty: 0.6886\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.16\n",
      "Sparsity with L1 penalty: 58.33%\n",
      "training data score with L1 penalty: 0.6911\n",
      "score with L1 penalty: 0.7441\n",
      "test data roc score with L1 penalty: 0.6896\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.17\n",
      "Sparsity with L1 penalty: 56.94%\n",
      "training data score with L1 penalty: 0.6905\n",
      "score with L1 penalty: 0.7435\n",
      "test data roc score with L1 penalty: 0.6900\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.18\n",
      "Sparsity with L1 penalty: 55.56%\n",
      "training data score with L1 penalty: 0.6907\n",
      "score with L1 penalty: 0.7438\n",
      "test data roc score with L1 penalty: 0.6902\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.19\n",
      "Sparsity with L1 penalty: 53.47%\n",
      "training data score with L1 penalty: 0.6910\n",
      "score with L1 penalty: 0.7440\n",
      "test data roc score with L1 penalty: 0.6908\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.20\n",
      "Sparsity with L1 penalty: 54.17%\n",
      "training data score with L1 penalty: 0.6909\n",
      "score with L1 penalty: 0.7429\n",
      "test data roc score with L1 penalty: 0.6900\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.21\n",
      "Sparsity with L1 penalty: 52.78%\n",
      "training data score with L1 penalty: 0.6906\n",
      "score with L1 penalty: 0.7438\n",
      "test data roc score with L1 penalty: 0.6905\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5682\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.22\n",
      "Sparsity with L1 penalty: 52.78%\n",
      "training data score with L1 penalty: 0.6905\n",
      "score with L1 penalty: 0.7431\n",
      "test data roc score with L1 penalty: 0.6893\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.23\n",
      "Sparsity with L1 penalty: 50.69%\n",
      "training data score with L1 penalty: 0.6911\n",
      "score with L1 penalty: 0.7431\n",
      "test data roc score with L1 penalty: 0.6893\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5682\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.24\n",
      "Sparsity with L1 penalty: 50.00%\n",
      "training data score with L1 penalty: 0.6913\n",
      "score with L1 penalty: 0.7422\n",
      "test data roc score with L1 penalty: 0.6886\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.25\n",
      "Sparsity with L1 penalty: 48.61%\n",
      "training data score with L1 penalty: 0.6906\n",
      "score with L1 penalty: 0.7420\n",
      "test data roc score with L1 penalty: 0.6884\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5682\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.26\n",
      "Sparsity with L1 penalty: 47.22%\n",
      "training data score with L1 penalty: 0.6913\n",
      "score with L1 penalty: 0.7416\n",
      "test data roc score with L1 penalty: 0.6886\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.27\n",
      "Sparsity with L1 penalty: 47.92%\n",
      "training data score with L1 penalty: 0.6916\n",
      "score with L1 penalty: 0.7405\n",
      "test data roc score with L1 penalty: 0.6880\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5683\n",
      "training data score with L2 penalty: 0.6011\n",
      "C=0.28\n",
      "Sparsity with L1 penalty: 45.83%\n",
      "training data score with L1 penalty: 0.6907\n",
      "score with L1 penalty: 0.7420\n",
      "test data roc score with L1 penalty: 0.6895\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.29\n",
      "Sparsity with L1 penalty: 44.44%\n",
      "training data score with L1 penalty: 0.6916\n",
      "score with L1 penalty: 0.7406\n",
      "test data roc score with L1 penalty: 0.6882\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n",
      "C=0.30\n",
      "Sparsity with L1 penalty: 43.75%\n",
      "training data score with L1 penalty: 0.6907\n",
      "score with L1 penalty: 0.7420\n",
      "test data roc score with L1 penalty: 0.6893\n",
      "Sparsity with L2 penalty: 4.86%\n",
      "score with L2 penalty: 0.5675\n",
      "training data score with L2 penalty: 0.6009\n"
     ]
    }
   ],
   "source": [
    "##logistical regression\n",
    "coef_l1_LR_all = []\n",
    "coef_l2_LR_all = []\n",
    "sparsity_l1_LR_all = []\n",
    "sparsity_l2_LR_all = []\n",
    "score_l1_LR_all = []\n",
    "score_l2_LR_all = []\n",
    "score_l1_LR_train_all = []\n",
    "score_l2_LR_train_all = []\n",
    "\n",
    "C_all = np.linspace(0.001,0.3, num=30)\n",
    "for i, C in enumerate(C_all):\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X_train_new2, y_train_new2)\n",
    "    clf_l2_LR.fit(X_train_new2, y_train_new2)\n",
    "    \n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "    coef_l1_LR_all.append(coef_l1_LR)\n",
    "    coef_l2_LR_all.append(coef_l2_LR)\n",
    "    \n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "    score_l1_LR_train = clf_l1_LR.score(X_train_new2,y_train_new2)\n",
    "    score_l2_LR_train = clf_l2_LR.score(X_train_new2,y_train_new2)\n",
    "    score_l1_LR = clf_l1_LR.score(X_test,y_test)\n",
    "    score_l2_LR = clf_l2_LR.score(X_test,y_test)\n",
    "    roc_l1_LR = roc_auc_score(y_test, clf_l1_LR.predict(X_test))\n",
    "\n",
    "    sparsity_l1_LR_all.append(sparsity_l1_LR)\n",
    "    sparsity_l2_LR_all.append(sparsity_l2_LR)\n",
    "    score_l1_LR_all.append(score_l1_LR)\n",
    "    score_l2_LR_all.append(score_l2_LR)\n",
    "    score_l1_LR_train_all.append(score_l1_LR_train)\n",
    "    score_l2_LR_train_all.append(score_l2_LR_train)\n",
    "    \n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"training data score with L1 penalty: %.4f\" % score_l1_LR_train)\n",
    "    print(\"score with L1 penalty: %.4f\" % score_l1_LR)\n",
    "    print(\"test data roc score with L1 penalty: %.4f\" % roc_l1_LR)\n",
    "\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % score_l2_LR)\n",
    "    print(\"training data score with L2 penalty: %.4f\" % score_l2_LR_train)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(C_all[1:30], sparsity_l1_LR_all[1:30], 'k')\n",
    "plt.plot(C_all[1:30], sparsity_l2_LR_all[1:30], 'r--')\n",
    "plt.grid(True)\n",
    "plt.figure(1)\n",
    "plt.subplot(212)\n",
    "plt.plot(C_all[1:30], score_l1_LR_train_all[1:30],'k')\n",
    "#plt.plot(C_all[1:30], score_l1_LR_train_all[1:30],'b')\n",
    "plt.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5791276547156885, 0.73656085864352594, 0.74345741036766388, 0.74633477963005257, 0.74770495546928517, 0.74756793788536191, 0.74569536423841054, 0.74638045215802695, 0.74537565654258964, 0.74496460379081986, 0.7451472939027175, 0.74505594884676862, 0.74487325873487098, 0.74368577300753602, 0.74437086092715232, 0.74414249828728019, 0.74350308289563827, 0.74382279059145928, 0.74400548070335693, 0.74290934003197073, 0.74382279059145928, 0.74309203014386849, 0.74309203014386849, 0.74217857958438005, 0.7419958894724823, 0.74158483672071251, 0.74053436857730071, 0.7419958894724823, 0.74062571363324958, 0.74195021694450791]\n"
     ]
    }
   ],
   "source": [
    "print(score_l1_LR_all)\n",
    "# from the score_l1_LR_all we can see the fifth value is highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747704955469\n",
      "['VehicleAge', 'VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice', 'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice', 'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice', 'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice', 'BYRNO', 'VehBCost', 'WarrantyCost', 'Auction_ADESA', 'Auction_MANHEIM', 'Make_CHEVROLET', 'Make_DODGE', 'Make_FORD', 'Color_BLUE', 'Color_RED', 'WheelTypeID_1.0', 'WheelTypeID_2.0', 'WheelTypeID_U0', 'Nationality_AMERICAN', 'Size_LARGE', 'Size_LARGE SUV', 'Size_MEDIUM', 'Size_MEDIUM SUV', 'Size_VAN', 'TopThreeAmericanName_GM', 'PRIMEUNIT_NO', 'AUCGUART_GREEN', 'VNST_FL', 'VNST_NC', 'VNST_TX']\n"
     ]
    }
   ],
   "source": [
    "#the best one the fifth time fitting\n",
    "list = np.where(coef_l1_LR_all[4] != 0)[0].tolist()\n",
    "head = X.columns.values.tolist()\n",
    "best_test_score = score_l1_LR_all[4]\n",
    "best_train_score = score_l1_LR_train_all[4]\n",
    "print(best_test_score)\n",
    "#head(list)\n",
    "need_head = []\n",
    "for i in list:\n",
    "    need_head.append(head[i])\n",
    "print(need_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
